{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "260a0e18",
      "metadata": {
        "id": "260a0e18"
      },
      "source": [
        "# 🪝 Multi‑Source Phishing URL Classifier (URL extractor edition)\n",
        "\n",
        "Each data source supplies **four pieces**:\n",
        "1. **path** to the file.\n",
        "2. **loader** – function that turns the file into a DataFrame.\n",
        "3. **url_extractor** – function that returns a `pd.Series` of URLs from that DataFrame.\n",
        "4. **label_extractor** – function that returns a binary phishing label (`1` = phish, `0` = benign).\n",
        "\n",
        "This makes heterogeneous schemas painless: you explicitly say where the URL and label live for every source.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "29e10d1c",
      "metadata": {
        "id": "29e10d1c"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tldextract'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtldextract\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbase64\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbinascii\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m urlparse, parse_qs\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tldextract'"
          ]
        }
      ],
      "source": [
        "#!pip install pandas scikit-learn tldextract pyarrow tqdm  # uncomment if needed\n",
        "#!pip install tldextract\n",
        "#!pip install -q tldextract pandas scikit-learn pyarrow tqdm\n",
        "\n",
        "import json, ipaddress, pathlib\n",
        "from typing import Callable, Union, List, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from tqdm import tqdm\n",
        "import tldextract\n",
        "import base64, binascii\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from sklearn.preprocessing import OneHotEncoder          # <-- one-hot\n",
        "from sklearn.feature_extraction import FeatureHasher     # <-- hashing trick\n",
        "from sklearn.ensemble import RandomForestClassifier      # <-- your model\n",
        "\n",
        "!pip install --quiet m2cgen\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb510bd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb510bd9",
        "outputId": "53c1101f-21ec-4397-cbc5-47fb3e4ccc8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registered 23 data source(s).\n"
          ]
        }
      ],
      "source": [
        "# 🔗 DATA SOURCES -----------------------------------------------------------------\n",
        "# Each tuple: (path, loader, url_extractor, label_extractor)\n",
        "\n",
        "def load_json(path: Union[str, pathlib.Path]):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return pd.DataFrame(json.load(f))\n",
        "\n",
        "def phishtank_urls(df: pd.DataFrame):\n",
        "    return df['url']\n",
        "\n",
        "def phishtank_label(df: pd.DataFrame):\n",
        "    # return df['verified'].astype(str).str.lower().isin({'yes','true','1'})\n",
        "    return True\n",
        "\n",
        "# Example benign CSV\n",
        "def load_csv(path):\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "def phish_score_url(df: pd.DataFrame):\n",
        "    return df['URL']  # adjust to your column name\n",
        "\n",
        "def phish_score_label(df: pd.DataFrame):\n",
        "    # return df['Score'].astype(int) > 4\n",
        "    return True\n",
        "\n",
        "SOURCES: List[Tuple[str, Callable, Callable, Callable]] = [\n",
        "    ('online-valid.json', load_json, phishtank_urls, phishtank_label),\n",
        "    ('phish_score.csv', load_csv, phish_score_url, phish_score_label),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_0.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_1.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_2.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_3.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_4.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_5.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_6.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_7.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_8.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_9.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_10.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_11.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_12.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_13.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_14.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_15.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_16.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_17.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_18.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_19.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_20.csv', load_csv, lambda df: df['url'], lambda df: False)\n",
        "]\n",
        "print(f'Registered {len(SOURCES)} data source(s).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lpJSKpkSS_66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpJSKpkSS_66",
        "outputId": "fb781402-7a57-41fe-eac3-61c9cf1b50f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d50d52",
      "metadata": {
        "id": "b6d50d52"
      },
      "outputs": [],
      "source": [
        "# 📂 Load & merge --------------------------------------------------------------\n",
        "frames = []\n",
        "for path, loader, url_fn, label_fn in SOURCES:\n",
        "    df = loader(path)\n",
        "    urls = url_fn(df)\n",
        "    labels = label_fn(df)\n",
        "    frame = pd.DataFrame({'url': urls, 'label': labels})\n",
        "    frames.append(frame)\n",
        "\n",
        "df_all = pd.concat(frames, ignore_index=True).dropna(subset=['url']).drop_duplicates()\n",
        "print(f'Combined dataset: {df_all.shape[0]:,} URLs')\n",
        "df_all.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a65037",
      "metadata": {
        "id": "77a65037"
      },
      "outputs": [],
      "source": [
        "# ✨ Feature engineering -------------------------------------------------------\n",
        "def url_length(u):\n",
        "    return len(u)\n",
        "\n",
        "def num_dashes(u):\n",
        "    return u.count('-')\n",
        "\n",
        "def num_dots(u):\n",
        "    return u.count('.')\n",
        "\n",
        "def num_subdirs(u):\n",
        "    return urlparse(u).path.count('/')\n",
        "\n",
        "def has_https(u):\n",
        "    return int(u.lower().startswith('https'))\n",
        "\n",
        "def domain_name(u):\n",
        "  return tldextract.extract(u).domain\n",
        "\n",
        "def domain_length(u):\n",
        "  return len(domain_name(u))\n",
        "\n",
        "def tld(u):\n",
        "  return tldextract.extract(u).suffix\n",
        "\n",
        "def sub_domain(u):\n",
        "  return tldextract.extract(u).subdomain\n",
        "\n",
        "def starts_with_tld(u):\n",
        "  tlds = ['com', 'org', 'net']\n",
        "  subdomains = sub_domain(u).split('.') + [domain_name(u)]\n",
        "  return int(any(sd.startswith(tld) for sd in subdomains for tld in tlds))\n",
        "\n",
        "def num_digits(u):\n",
        "  return sum(c.isdigit() for c in u)\n",
        "\n",
        "def num_letters(u):\n",
        "  return sum(c.isalpha() for c in u)\n",
        "\n",
        "def params_length(u):\n",
        "  return len(urlparse(u).query)\n",
        "\n",
        "def num_params(u):\n",
        "  return len(parse_qs(urlparse(u).query))\n",
        "\n",
        "def _looks_like_base64(s: str) -> bool:\n",
        "    \"\"\"\n",
        "    True if the entire string is valid Base-64.\n",
        "    - Must be non-empty and length a multiple of 4.\n",
        "    - `validate=True` rejects non-alphabet chars.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        base64.b64decode(s, validate=True)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def has_b64_param(u: str) -> int:\n",
        "    \"\"\"\n",
        "    Return 1 if **any query-string value** is valid Base-64; else 0.\n",
        "    Example: https://example.com/?img=aGVsbG8=  → 1\n",
        "    \"\"\"\n",
        "    qs_values = sum(parse_qs(urlparse(u).query, keep_blank_values=True).values(), [])\n",
        "    return int(any(_looks_like_base64(v) for v in qs_values))\n",
        "\n",
        "def _looks_url(s):\n",
        "  indicators = ['com', 'org', 'net', 'http', 'www']\n",
        "  return any(indicator in s for indicator in indicators)\n",
        "\n",
        "def has_url_in_params(u: str) -> int:\n",
        "    qs_values = sum(parse_qs(urlparse(u).query, keep_blank_values=True).values(), [])\n",
        "    return int(any(_looks_url(v) for v in qs_values))\n",
        "\n",
        "\n",
        "def uses_ip_address(u):\n",
        "    host = urlparse(u).hostname or ''\n",
        "    try:\n",
        "        ipaddress.ip_address(host)\n",
        "        return 1\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "FEATURES = {\n",
        "    # ── length / composition ────────────────────────────────────────────────\n",
        "    \"url_length\":       url_length,\n",
        "    \"num_dashes\":       num_dashes,\n",
        "    \"num_dots\":         num_dots,\n",
        "    \"num_subdirs\":      num_subdirs,\n",
        "    \"num_digits\":       num_digits,\n",
        "    \"num_letters\":      num_letters,\n",
        "    \"domain_length\":    domain_length,\n",
        "    \"params_length\":    params_length,\n",
        "    \"num_params\":       num_params,\n",
        "\n",
        "    # ── boolean / flag features ─────────────────────────────────────────────\n",
        "    \"has_https\":         has_https,\n",
        "    \"uses_ip_address\":   uses_ip_address,\n",
        "    \"starts_with_tld\":   starts_with_tld,\n",
        "    \"has_b64_param\":     has_b64_param,\n",
        "    \"has_url_in_params\": has_url_in_params,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa75078",
      "metadata": {
        "id": "2fa75078"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "def build_X(df, funcs):\n",
        "    \"\"\"numeric feature matrix\"\"\"\n",
        "    return pd.DataFrame({name: [f(u) for u in df['url']] for name, f in funcs.items()})\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# assume df_all with columns ['url','label'] already exists\n",
        "df_all = df_all.reset_index(drop=True)\n",
        "X_num = build_X(df_all, FEATURES)\n",
        "\n",
        "# raw categorical columns\n",
        "cat_df = pd.DataFrame({\n",
        "    \"tld\":         [tld(u)         for u in df_all[\"url\"]],\n",
        "    \"domain_name\": [domain_name(u) for u in df_all[\"url\"]],\n",
        "    \"sub_domain\":  [sub_domain(u)  for u in df_all[\"url\"]],\n",
        "})\n",
        "\n",
        "# one-hot for TLD\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "X_tld = pd.DataFrame(\n",
        "    ohe.fit_transform(cat_df[[\"tld\"]]),\n",
        "    index=df_all.index,\n",
        "    columns=ohe.get_feature_names_out([\"tld\"]),\n",
        ")\n",
        "\n",
        "# 2) hash *domain_name* into 32 dims ------------------------------------------\n",
        "hasher_dom = FeatureHasher(n_features = 1024, input_type=\"string\")\n",
        "dom_iter   = [[d] for d in cat_df[\"domain_name\"].astype(str)]\n",
        "X_dom_hash = hasher_dom.transform(dom_iter).toarray()\n",
        "X_dom_hash = pd.DataFrame(\n",
        "    X_dom_hash, index=df_all.index,\n",
        "    columns=[f\"dom_hash_{i}\" for i in range(X_dom_hash.shape[1])]\n",
        ")\n",
        "\n",
        "# 3) hash *sub_domain* into 32 dims -------------------------------------------\n",
        "hasher_sub = FeatureHasher(n_features = 1024, input_type=\"string\")\n",
        "sub_iter   = [[s if s else \"EMPTY\"] for s in cat_df[\"sub_domain\"].astype(str)]\n",
        "X_sub_hash = hasher_sub.transform(sub_iter).toarray()\n",
        "X_sub_hash = pd.DataFrame(\n",
        "    X_sub_hash, index=df_all.index,\n",
        "    columns=[f\"sub_hash_{i}\" for i in range(X_sub_hash.shape[1])]\n",
        ")\n",
        "\n",
        "# 4) final matrix -------------------------------------------------------------\n",
        "X_full = pd.concat([X_num, X_tld, X_dom_hash, X_sub_hash], axis=1)\n",
        "y      = df_all[\"label\"]\n",
        "print(\"Final shape:\", X_full.shape)   # (rows,  numeric + one-hot + 64 hashed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f9e4e23",
      "metadata": {
        "id": "4f9e4e23"
      },
      "outputs": [],
      "source": [
        "# ⚙️  Algorithms ----------------------------------------------------------------\n",
        "MODELS = {\n",
        "    #'LogReg': LogisticRegression(max_iter=1000),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=200),\n",
        "    'GradientBoosting': GradientBoostingClassifier()\n",
        "    #'SVM_rbf': SVC(kernel='rbf', probability=True),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd9ed239",
      "metadata": {
        "id": "bd9ed239"
      },
      "outputs": [],
      "source": [
        "# 🧪 Hold-out evaluation ------------------------------------------------------\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import train_test_split  # ← make sure this is imported\n",
        "\n",
        "def evaluate(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    probs = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, preds, labels=[0, 1]).ravel()\n",
        "    tpr = tp / (tp + fn) if (tp + fn) else 0\n",
        "    fpr = fp / (fp + tn) if (fp + tn) else 0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, preds),\n",
        "        \"f1\": f1_score(y_test, preds),\n",
        "        \"roc_auc\": roc_auc_score(y_test, probs) if probs is not None else np.nan,\n",
        "        \"tpr\": tpr,\n",
        "        \"fpr\": fpr,\n",
        "    }\n",
        "\n",
        "# split and score  (note X_full, not X)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X_full, y, test_size=0.30, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "results = {\n",
        "    name: evaluate(mdl, X_tr, X_te, y_tr, y_te)\n",
        "    for name, mdl in MODELS.items()\n",
        "}\n",
        "\n",
        "pd.DataFrame(results).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00c96cc5",
      "metadata": {
        "id": "00c96cc5"
      },
      "outputs": [],
      "source": [
        "# 📊 5-fold cross-validation ---------------------------------------------------\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rows = []\n",
        "for fold, (tr, te) in enumerate(skf.split(X_full, y)):\n",
        "    X_tr, X_te = X_full.iloc[tr], X_full.iloc[te]\n",
        "    y_tr, y_te = y.iloc[tr], y.iloc[te]\n",
        "\n",
        "    for name, mdl in MODELS.items():\n",
        "        rows.append({\n",
        "            \"model\": name,\n",
        "            \"fold\": fold,\n",
        "            **evaluate(mdl, X_tr, X_te, y_tr, y_te)   # accuracy, f1, roc_auc, tpr, fpr\n",
        "        })\n",
        "\n",
        "cv_df = pd.DataFrame(rows)\n",
        "\n",
        "metrics = [\"accuracy\", \"f1\", \"roc_auc\", \"tpr\", \"fpr\"]\n",
        "cv_df.groupby(\"model\")[metrics].mean()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab456eea",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ab456eea"
      },
      "outputs": [],
      "source": [
        "# 🌳 Feature importance --------------------------------------------------------\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=300, random_state=42).fit(X_full, y)\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=X_full.columns) \\\n",
        "               .sort_values(ascending=False)\n",
        "\n",
        "importances.head(20)    # show the top 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3710c0c",
      "metadata": {
        "id": "d3710c0c"
      },
      "source": [
        "## 🚀 Next Steps\n",
        "* Add more `(path, loader, url_extractor, label_extractor)` tuples to `SOURCES`.\n",
        "* Engineer more `FEATURES`.\n",
        "* Try additional algorithms or hyper‑parameter tuning.\n",
        "* Address class imbalance if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9hmZzcVBTVzo",
      "metadata": {
        "id": "9hmZzcVBTVzo"
      },
      "outputs": [],
      "source": [
        "max_depths = [est.tree_.max_depth for est in rf.estimators_]\n",
        "print(\"deepest tree:\", max(max_depths))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77c812a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn-porter in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.7.4)\n",
            "Requirement already satisfied: six in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sklearn-porter) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn>=0.14.1 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sklearn-porter) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.14.1->sklearn-porter) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.14.1->sklearn-porter) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.14.1->sklearn-porter) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.14.1->sklearn-porter) (3.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: m2cgen in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from m2cgen) (1.24.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: onnx 1.14.1\n",
            "Uninstalling onnx-1.14.1:\n",
            "  Successfully uninstalled onnx-1.14.1\n",
            "Found existing installation: skl2onnx 1.14.0\n",
            "Uninstalling skl2onnx-1.14.0:\n",
            "  Successfully uninstalled skl2onnx-1.14.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting skl2onnx==1.14\n",
            "  Using cached skl2onnx-1.14.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting onnx==1.14.1\n",
            "  Using cached onnx-1.14.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
            "Collecting onnxmltools==1.11\n",
            "  Downloading onnxmltools-1.11.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: scikit-learn<1.3,>=0.19 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from skl2onnx==1.14) (1.2.2)\n",
            "Requirement already satisfied: onnxconverter-common>=1.7.0 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from skl2onnx==1.14) (1.13.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnx==1.14.1) (1.24.3)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnx==1.14.1) (6.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnx==1.14.1) (4.12.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxconverter-common>=1.7.0->skl2onnx==1.14) (24.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn<1.3,>=0.19->skl2onnx==1.14) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn<1.3,>=0.19->skl2onnx==1.14) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\omer penso\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn<1.3,>=0.19->skl2onnx==1.14) (3.5.0)\n",
            "Using cached skl2onnx-1.14.0-py2.py3-none-any.whl (294 kB)\n",
            "Using cached onnx-1.14.1-cp310-cp310-win_amd64.whl (13.3 MB)\n",
            "Downloading onnxmltools-1.11.0-py2.py3-none-any.whl (302 kB)\n",
            "Installing collected packages: onnx, skl2onnx, onnxmltools\n",
            "Successfully installed onnx-1.14.1 onnxmltools-1.11.0 skl2onnx-1.14.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install sklearn-porter\n",
        "%pip install m2cgen\n",
        "%pip install skl2onnx==1.14 onnx==1.14.1 onnxmltools==1.11\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iYh0BDPvTWW6",
      "metadata": {
        "id": "iYh0BDPvTWW6"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'split_complex_to_pairs' from 'onnx.helper' (c:\\Users\\omer penso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\onnx\\helper.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mm2cgen\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mm2c\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mskl2onnx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_sklearn\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mskl2onnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FloatTensorType\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\omer penso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skl2onnx\\__init__.py:15\u001b[0m\n\u001b[0;32m     11\u001b[0m __model_version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m __max_supported_opset__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m  \u001b[38;5;66;03m# Converters are tested up to this version.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_sklearn, to_onnx, wrap_as_onnx_mixin  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supported_operators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     update_registered_converter, get_model_alias\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m update_registered_parser  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\omer penso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skl2onnx\\convert.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01muuid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m uuid4\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_latest_tested_opset_version\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_topology\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_topology\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils_sklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _process_options\n",
            "File \u001b[1;32mc:\\Users\\omer penso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skl2onnx\\proto\\__init__.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# onnx is too old.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_complex_to_pairs\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_tensor_fixed\u001b[39m(name, data_type, dims, vals, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     24\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    Make a TensorProto with specified arguments.  If raw is False, this\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    function will choose the corresponding proto field to store the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    this case.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'split_complex_to_pairs' from 'onnx.helper' (c:\\Users\\omer penso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\onnx\\helper.py)"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn_porter import Porter\n",
        "import m2cgen as m2c, sys\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType\n",
        "import joblib\n",
        "\n",
        "sys.setrecursionlimit(20000)                # still raise the limit\n",
        "\n",
        "rf_shallow = RandomForestClassifier(\n",
        "    n_estimators=200,        # you can keep 300 if you like\n",
        "    max_depth=12,           # <- key change: limit depth\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ").fit(X_tr, y_tr)\n",
        "\n",
        "\n",
        "# Convert model to JavaScript\n",
        "porter = Porter(rf_shallow, language='js')\n",
        "js_code = porter.export(embed_data=True)\n",
        "\n",
        "# Save to file\n",
        "with open('rf_shallow.js', 'w') as f:\n",
        "    f.write(js_code)\n",
        "\n",
        "# joblib.dump(rf_shallow, \"rf_model.pkl\")\n",
        "\n",
        "# model = joblib.load(\"rf_model.pkl\")\n",
        "\n",
        "# # Set input type (adjust input dimension)\n",
        "# n_features = X_tr.shape[1]\n",
        "# initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
        "\n",
        "# # Convert to ONNX\n",
        "# onnx_model = convert_sklearn(model, initial_types=initial_type)\n",
        "\n",
        "# # Save ONNX model\n",
        "# with open(\"rf_model.onnx\", \"wb\") as f:\n",
        "#     f.write(onnx_model.SerializeToString())\n",
        "\n",
        "# I'm commenting to try another porter\n",
        "# js_model_code = m2c.export_to_javascript(rf_shallow,\n",
        "#                                          function_name=\"predictRF\")\n",
        "\n",
        "# with open(\"rf_model.js\", \"w\") as f:\n",
        "#     f.write(js_model_code)\n",
        "\n",
        "# print(\"JS model saved, size ≈\", len(js_model_code)//1024, \"KB\")\n",
        "\n",
        "# print(list(ohe.get_feature_names_out(['tld'])))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f233b687",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
