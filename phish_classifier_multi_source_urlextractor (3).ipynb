{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "260a0e18",
      "metadata": {
        "id": "260a0e18"
      },
      "source": [
        "# ğŸª Multiâ€‘Source Phishingâ€¯URL Classifier (URL extractor edition)\n",
        "\n",
        "Each data source supplies **four pieces**:\n",
        "1. **path** to the file.\n",
        "2. **loader** â€“ function that turns the file into a DataFrame.\n",
        "3. **url_extractor** â€“ function that returns a `pd.Series` of URLs from that DataFrame.\n",
        "4. **label_extractor** â€“ function that returns a binary phishing label (`1`Â =Â phish, `0`Â =Â benign).\n",
        "\n",
        "This makes heterogeneous schemas painless: you explicitly say where the URL and label live for every source.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "29e10d1c",
      "metadata": {
        "id": "29e10d1c"
      },
      "outputs": [],
      "source": [
        "#!pip install pandas scikit-learn tldextract pyarrow tqdm  # uncomment if needed\n",
        "#!pip install tldextract\n",
        "#!pip install -q tldextract pandas scikit-learn pyarrow tqdm\n",
        "\n",
        "import json, ipaddress, pathlib\n",
        "from typing import Callable, Union, List, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from tqdm import tqdm\n",
        "import tldextract\n",
        "import base64, binascii\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from sklearn.preprocessing import OneHotEncoder          # <-- one-hot\n",
        "from sklearn.feature_extraction import FeatureHasher     # <-- hashing trick\n",
        "from sklearn.ensemble import RandomForestClassifier      # <-- your model\n",
        "\n",
        "!pip install --quiet m2cgen\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fb510bd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb510bd9",
        "outputId": "53c1101f-21ec-4397-cbc5-47fb3e4ccc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registered 23 data source(s).\n"
          ]
        }
      ],
      "source": [
        "# ğŸ”— DATA SOURCES -----------------------------------------------------------------\n",
        "# Each tuple: (path, loader, url_extractor, label_extractor)\n",
        "\n",
        "def load_json(path: Union[str, pathlib.Path]):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return pd.DataFrame(json.load(f))\n",
        "\n",
        "def phishtank_urls(df: pd.DataFrame):\n",
        "    return df['url']\n",
        "\n",
        "def phishtank_label(df: pd.DataFrame):\n",
        "    # return df['verified'].astype(str).str.lower().isin({'yes','true','1'})\n",
        "    return True\n",
        "\n",
        "# Example benign CSV\n",
        "def load_csv(path):\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "def phish_score_url(df: pd.DataFrame):\n",
        "    return df['URL']  # adjust to your column name\n",
        "\n",
        "def phish_score_label(df: pd.DataFrame):\n",
        "    # return df['Score'].astype(int) > 4\n",
        "    return True\n",
        "\n",
        "SOURCES: List[Tuple[str, Callable, Callable, Callable]] = [\n",
        "    ('online-valid.json', load_json, phishtank_urls, phishtank_label),\n",
        "    ('phish_score.csv', load_csv, phish_score_url, phish_score_label),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_0.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_1.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_2.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_3.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_4.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_5.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_6.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_7.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_8.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_9.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_10.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_11.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_12.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_13.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_14.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_15.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_16.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_17.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_18.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_19.csv', load_csv, lambda df: df['url'], lambda df: False),\n",
        "    ('/content/drive/MyDrive/Benign Samples/benign_dataframe_20.csv', load_csv, lambda df: df['url'], lambda df: False)\n",
        "]\n",
        "print(f'Registered {len(SOURCES)} data source(s).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "lpJSKpkSS_66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpJSKpkSS_66",
        "outputId": "fb781402-7a57-41fe-eac3-61c9cf1b50f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d50d52",
      "metadata": {
        "id": "b6d50d52"
      },
      "outputs": [],
      "source": [
        "# ğŸ“‚ Load & merge --------------------------------------------------------------\n",
        "frames = []\n",
        "for path, loader, url_fn, label_fn in SOURCES:\n",
        "    df = loader(path)\n",
        "    urls = url_fn(df)\n",
        "    labels = label_fn(df)\n",
        "    frame = pd.DataFrame({'url': urls, 'label': labels})\n",
        "    frames.append(frame)\n",
        "\n",
        "df_all = pd.concat(frames, ignore_index=True).dropna(subset=['url']).drop_duplicates()\n",
        "print(f'Combined dataset: {df_all.shape[0]:,} URLs')\n",
        "df_all.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a65037",
      "metadata": {
        "id": "77a65037"
      },
      "outputs": [],
      "source": [
        "# âœ¨ Feature engineering -------------------------------------------------------\n",
        "def url_length(u):\n",
        "    return len(u)\n",
        "\n",
        "def num_dashes(u):\n",
        "    return u.count('-')\n",
        "\n",
        "def num_dots(u):\n",
        "    return u.count('.')\n",
        "\n",
        "def num_subdirs(u):\n",
        "    return urlparse(u).path.count('/')\n",
        "\n",
        "def has_https(u):\n",
        "    return int(u.lower().startswith('https'))\n",
        "\n",
        "def domain_name(u):\n",
        "  return tldextract.extract(u).domain\n",
        "\n",
        "def domain_length(u):\n",
        "  return len(domain_name(u))\n",
        "\n",
        "def tld(u):\n",
        "  return tldextract.extract(u).suffix\n",
        "\n",
        "def sub_domain(u):\n",
        "  return tldextract.extract(u).subdomain\n",
        "\n",
        "def starts_with_tld(u):\n",
        "  tlds = ['com', 'org', 'net']\n",
        "  subdomains = sub_domain(u).split('.') + [domain_name(u)]\n",
        "  return int(any(sd.startswith(tld) for sd in subdomains for tld in tlds))\n",
        "\n",
        "def num_digits(u):\n",
        "  return sum(c.isdigit() for c in u)\n",
        "\n",
        "def num_letters(u):\n",
        "  return sum(c.isalpha() for c in u)\n",
        "\n",
        "def params_length(u):\n",
        "  return len(urlparse(u).query)\n",
        "\n",
        "def num_params(u):\n",
        "  return len(parse_qs(urlparse(u).query))\n",
        "\n",
        "def _looks_like_base64(s: str) -> bool:\n",
        "    \"\"\"\n",
        "    True if the entire string is valid Base-64.\n",
        "    - Must be non-empty and length a multiple of 4.\n",
        "    - `validate=True` rejects non-alphabet chars.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        base64.b64decode(s, validate=True)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def has_b64_param(u: str) -> int:\n",
        "    \"\"\"\n",
        "    Return 1 if **any query-string value** is valid Base-64; else 0.\n",
        "    Example: https://example.com/?img=aGVsbG8=  â†’ 1\n",
        "    \"\"\"\n",
        "    qs_values = sum(parse_qs(urlparse(u).query, keep_blank_values=True).values(), [])\n",
        "    return int(any(_looks_like_base64(v) for v in qs_values))\n",
        "\n",
        "def _looks_url(s):\n",
        "  indicators = ['com', 'org', 'net', 'http', 'www']\n",
        "  return any(indicator in s for indicator in indicators)\n",
        "\n",
        "def has_url_in_params(u: str) -> int:\n",
        "    qs_values = sum(parse_qs(urlparse(u).query, keep_blank_values=True).values(), [])\n",
        "    return int(any(_looks_url(v) for v in qs_values))\n",
        "\n",
        "\n",
        "def uses_ip_address(u):\n",
        "    host = urlparse(u).hostname or ''\n",
        "    try:\n",
        "        ipaddress.ip_address(host)\n",
        "        return 1\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "FEATURES = {\n",
        "    # â”€â”€ length / composition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    \"url_length\":       url_length,\n",
        "    \"num_dashes\":       num_dashes,\n",
        "    \"num_dots\":         num_dots,\n",
        "    \"num_subdirs\":      num_subdirs,\n",
        "    \"num_digits\":       num_digits,\n",
        "    \"num_letters\":      num_letters,\n",
        "    \"domain_length\":    domain_length,\n",
        "    \"params_length\":    params_length,\n",
        "    \"num_params\":       num_params,\n",
        "\n",
        "    # â”€â”€ boolean / flag features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    \"has_https\":         has_https,\n",
        "    \"uses_ip_address\":   uses_ip_address,\n",
        "    \"starts_with_tld\":   starts_with_tld,\n",
        "    \"has_b64_param\":     has_b64_param,\n",
        "    \"has_url_in_params\": has_url_in_params,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa75078",
      "metadata": {
        "id": "2fa75078"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "def build_X(df, funcs):\n",
        "    \"\"\"numeric feature matrix\"\"\"\n",
        "    return pd.DataFrame({name: [f(u) for u in df['url']] for name, f in funcs.items()})\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# assume df_all with columns ['url','label'] already exists\n",
        "df_all = df_all.reset_index(drop=True)\n",
        "X_num = build_X(df_all, FEATURES)\n",
        "\n",
        "# raw categorical columns\n",
        "cat_df = pd.DataFrame({\n",
        "    \"tld\":         [tld(u)         for u in df_all[\"url\"]],\n",
        "    \"domain_name\": [domain_name(u) for u in df_all[\"url\"]],\n",
        "    \"sub_domain\":  [sub_domain(u)  for u in df_all[\"url\"]],\n",
        "})\n",
        "\n",
        "# one-hot for TLD\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "X_tld = pd.DataFrame(\n",
        "    ohe.fit_transform(cat_df[[\"tld\"]]),\n",
        "    index=df_all.index,\n",
        "    columns=ohe.get_feature_names_out([\"tld\"]),\n",
        ")\n",
        "\n",
        "# 2) hash *domain_name* into 32 dims ------------------------------------------\n",
        "hasher_dom = FeatureHasher(n_features = 1024, input_type=\"string\")\n",
        "dom_iter   = [[d] for d in cat_df[\"domain_name\"].astype(str)]\n",
        "X_dom_hash = hasher_dom.transform(dom_iter).toarray()\n",
        "X_dom_hash = pd.DataFrame(\n",
        "    X_dom_hash, index=df_all.index,\n",
        "    columns=[f\"dom_hash_{i}\" for i in range(X_dom_hash.shape[1])]\n",
        ")\n",
        "\n",
        "# 3) hash *sub_domain* into 32 dims -------------------------------------------\n",
        "hasher_sub = FeatureHasher(n_features = 1024, input_type=\"string\")\n",
        "sub_iter   = [[s if s else \"EMPTY\"] for s in cat_df[\"sub_domain\"].astype(str)]\n",
        "X_sub_hash = hasher_sub.transform(sub_iter).toarray()\n",
        "X_sub_hash = pd.DataFrame(\n",
        "    X_sub_hash, index=df_all.index,\n",
        "    columns=[f\"sub_hash_{i}\" for i in range(X_sub_hash.shape[1])]\n",
        ")\n",
        "\n",
        "# 4) final matrix -------------------------------------------------------------\n",
        "X_full = pd.concat([X_num, X_tld, X_dom_hash, X_sub_hash], axis=1)\n",
        "y      = df_all[\"label\"]\n",
        "print(\"Final shape:\", X_full.shape)   # (rows,  numeric + one-hot + 64 hashed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f9e4e23",
      "metadata": {
        "id": "4f9e4e23"
      },
      "outputs": [],
      "source": [
        "# âš™ï¸  Algorithms ----------------------------------------------------------------\n",
        "MODELS = {\n",
        "    #'LogReg': LogisticRegression(max_iter=1000),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=200),\n",
        "    'GradientBoosting': GradientBoostingClassifier()\n",
        "    #'SVM_rbf': SVC(kernel='rbf', probability=True),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd9ed239",
      "metadata": {
        "id": "bd9ed239"
      },
      "outputs": [],
      "source": [
        "# ğŸ§ª Hold-out evaluation ------------------------------------------------------\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import train_test_split  # â† make sure this is imported\n",
        "\n",
        "def evaluate(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    probs = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, preds, labels=[0, 1]).ravel()\n",
        "    tpr = tp / (tp + fn) if (tp + fn) else 0\n",
        "    fpr = fp / (fp + tn) if (fp + tn) else 0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_test, preds),\n",
        "        \"f1\": f1_score(y_test, preds),\n",
        "        \"roc_auc\": roc_auc_score(y_test, probs) if probs is not None else np.nan,\n",
        "        \"tpr\": tpr,\n",
        "        \"fpr\": fpr,\n",
        "    }\n",
        "\n",
        "# split and score  (note X_full, not X)\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X_full, y, test_size=0.30, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "results = {\n",
        "    name: evaluate(mdl, X_tr, X_te, y_tr, y_te)\n",
        "    for name, mdl in MODELS.items()\n",
        "}\n",
        "\n",
        "pd.DataFrame(results).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00c96cc5",
      "metadata": {
        "id": "00c96cc5"
      },
      "outputs": [],
      "source": [
        "# ğŸ“Š 5-fold cross-validation ---------------------------------------------------\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rows = []\n",
        "for fold, (tr, te) in enumerate(skf.split(X_full, y)):\n",
        "    X_tr, X_te = X_full.iloc[tr], X_full.iloc[te]\n",
        "    y_tr, y_te = y.iloc[tr], y.iloc[te]\n",
        "\n",
        "    for name, mdl in MODELS.items():\n",
        "        rows.append({\n",
        "            \"model\": name,\n",
        "            \"fold\": fold,\n",
        "            **evaluate(mdl, X_tr, X_te, y_tr, y_te)   # accuracy, f1, roc_auc, tpr, fpr\n",
        "        })\n",
        "\n",
        "cv_df = pd.DataFrame(rows)\n",
        "\n",
        "metrics = [\"accuracy\", \"f1\", \"roc_auc\", \"tpr\", \"fpr\"]\n",
        "cv_df.groupby(\"model\")[metrics].mean()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab456eea",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ab456eea"
      },
      "outputs": [],
      "source": [
        "# ğŸŒ³ Feature importance --------------------------------------------------------\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=300, random_state=42).fit(X_full, y)\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=X_full.columns) \\\n",
        "               .sort_values(ascending=False)\n",
        "\n",
        "importances.head(20)    # show the top 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3710c0c",
      "metadata": {
        "id": "d3710c0c"
      },
      "source": [
        "## ğŸš€ Next Steps\n",
        "* Add more `(path, loader, url_extractor, label_extractor)` tuples to `SOURCES`.\n",
        "* Engineer more `FEATURES`.\n",
        "* Try additional algorithms or hyperâ€‘parameter tuning.\n",
        "* Address class imbalance if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9hmZzcVBTVzo",
      "metadata": {
        "id": "9hmZzcVBTVzo"
      },
      "outputs": [],
      "source": [
        "max_depths = [est.tree_.max_depth for est in rf.estimators_]\n",
        "print(\"deepest tree:\", max(max_depths))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iYh0BDPvTWW6",
      "metadata": {
        "id": "iYh0BDPvTWW6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import m2cgen as m2c, sys\n",
        "\n",
        "sys.setrecursionlimit(20000)                # still raise the limit\n",
        "\n",
        "rf_shallow = RandomForestClassifier(\n",
        "    n_estimators=200,        # you can keep 300 if you like\n",
        "    max_depth=12,           # <- key change: limit depth\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ").fit(X_tr, y_tr)\n",
        "\n",
        "js_model_code = m2c.export_to_javascript(rf_shallow,\n",
        "                                         function_name=\"predictRF\")\n",
        "\n",
        "with open(\"rf_model.js\", \"w\") as f:\n",
        "    f.write(js_model_code)\n",
        "\n",
        "print(\"JS model saved, size â‰ˆ\", len(js_model_code)//1024, \"KB\")\n",
        "\n",
        "print(list(ohe.get_feature_names_out(['tld'])))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}